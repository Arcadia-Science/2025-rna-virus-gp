# TODO: Replace with the name of the repo

[![run with conda](https://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/projects/miniconda/en/latest/)

Note: Analysis repo names should be prefixed with the year (ie `2024-noveltree-analysis`)

## Purpose

This repository contains the complete workflow for validating the G-P Atlas framework on complex, simulated RNA virus datasets. The core motivation is to move beyond statistical validation and rigorously test the model's ability to learn specific, biologically-grounded causal mechanisms.

A central challenge in modern biology is deciphering the nonlinear mapping between genotype and phenotype. The G-P Atlas was developed to address this by modeling epistasis and pleiotropy in a two-tiered neural network architecture. This project builds upon that foundation by developing a custom, deterministic simulation framework to generate synthetic viral populations with precisely known G-P maps.

The core analyses performed in this repository are:

Deterministic Data Generation: Creation of biologically realistic SARS-CoV-2 and Influenza A virus datasets where the ground-truth G-P map (including epistasis, pleiotropy, and phenotype-phenotype dependencies) is explicitly defined.

Model Training & Hyperparameter Sweeps: Training the G-P Atlas model on these deterministic datasets under a range of experimental conditions (e.g., varying noise levels and latent dimension sizes).

Performance Analysis & Interpretation: A deep analysis of the model's performance to understand its architectural strengths and weaknesses, specifically its ability to learn different types of biological complexity.

## Installation and Setup

This repository uses conda to manage software environments and installations. You can find operating system-specific instructions for installing miniconda [here](https://docs.conda.io/projects/miniconda/en/latest/). After installing conda and [mamba](https://mamba.readthedocs.io/en/latest/), run the following command to create the pipeline run environment.

```{bash}
TODO: Replace <NAME> with the name of your environment
mamba env create -n rna_virus_gp_env --file envs/dev.yml
conda activate rna_virus_gp_env
```

<details><summary>Developer Notes (click to expand/collapse)</summary>

1. Install your pre-commit hooks:

    ```{bash}
    pre-commit install
    ```

    This installs the pre-commit hooks defined in your config (`./.pre-commit-config.yaml`).

2. Export your conda environment before sharing:

    As your project develops, the number of dependencies in your environment may increase. Whenever you install new dependencies (using either `pip install` or `mamba install`), you should update the environment file using the following command.

    ```{bash}
    conda env export --no-builds > envs/dev.yml
    ```

    `--no-builds` removes build specification from the exported packages to increase portability between different platforms.
</details>

## Data

This project utilizes a custom data generation pipeline. The data is not stored directly in the repository but is generated by the user following the workflow described in the Methods section.

Input Data
The primary inputs to the data generation pipeline are the G-P Map Definition Files, located in the /maps directory. These are JSON files that declaratively define the entire causal network for a given simulation.

cov_map_v4.json: Defines the G-P map for the complex SARS-CoV-2 simulation.

iav_map_v3.json: Defines the G-P map for the Influenza A virus simulation.

Each map specifies the number of loci and phenotypes, and for each phenotype, it defines its causal inputs: additive loci, interactive (epistatic) loci, and dependent phenotypes. The maps also include a custom_weights section to assign fixed, high-impact effects to specific mutations.

Output Data
The workflow generates two main types of output data:

Generated Datasets: The map_to_phenotype_v2.py script produces training and testing datasets, saved as pickle files (.pk) in the /deterministic_datasets directory. Each pickle file is a dictionary containing the one-hot encoded genotypes matrix and the corresponding phenotypes matrix.

Model Training Results: The g_p_atlas_v2.py script saves its outputs to the /results directory. For each experimental run, a new folder is created (e.g., /results/cov_v4_latent_10/noise_01/) which contains:

test_stats.pk: A pickle file with the final performance metrics (including RÂ² scores) for the run.

*.pt: Saved PyTorch model weights for the encoders and decoder.

*.svg: Plots generated by the script, such as the real_vs_predicted scatter plot.
## Overview

### Description of the folder structure

### Methods

The project is structured as a four-step pipeline, moving from simulation design to final analysis.

1. Simulation Design
The G-P maps are defined in the JSON files within the /maps directory. These can be edited to create new simulations with different causal structures.

2. Data Generation
Use the custom deterministic simulator to generate the training and testing datasets from a JSON map.

Example Command:

python scripts/map_to_phenotype_v2.py \
    --map_file ./maps/cov_map_v4.json \
    --out_dir ./deterministic_datasets/cov_v4 \
    --n_individuals 10000 \
    --noise 0.1

3. Model Training
Use the g_p_atlas_v2.py script to train the model on a generated dataset. The script requires a dedicated directory for its outputs, so the recommended workflow is to create a new directory for each run and copy the data into it.

Example Command:

# Create a directory for the experiment
mkdir -p ./results/cov_v4_latent_10/noise_01

# Copy the data
cp ./deterministic_datasets/cov_v4/train_data.pk ./results/cov_v4_latent_10/noise_01/
cp ./deterministic_datasets/cov_v4/test_data.pk ./results/cov_v4_latent_10/noise_01/

# Run the training
python scripts/g_p_atlas_v2.py \
    --dataset_path ./results/cov_v4_latent_10/noise_01/ \
    --train_suffix train_data.pk \
    --test_suffix test_data.pk \
    --n_alleles 3 \
    --n_loci_measured 53 \
    --n_phens 16 \
    --n_phens_to_analyze 16 \
    --n_phens_to_predict 16 \
    --latent_dim 10 \
    --n_epochs 200 \
    --n_epochs_gen 200 \
    --batch_size 64 \
    --sd_noise 0.1

4. Analysis and Visualization
All post-training analysis is performed in the analysis.ipynb Jupyter Notebook. This notebook loads the results from all experimental runs and generates the final plots and summary statistics, including:

Noise and latent dimension sensitivity analyses.

Per-phenotype performance plots.

A quantitative comparison of our deterministic map's complexity vs. the original stochastic generator.

A network visualization of the G-P map.

### Compute Specifications

The analyses were performed on a MacBook Pro with an Apple M3 Max chip and 36GB of RAM. Training time for a single experimental run (200 epochs) was approximately 15-25 minutes.

## Contributing

See how we recognize [feedback and contributions to our code](https://github.com/Arcadia-Science/arcadia-software-handbook/blob/main/guides-and-standards/guide--credit-for-contributions.md).

---
## For Developers

This section contains information for developers who are working off of this template. Please adjust or edit this section as appropriate when you're ready to share your repo.

### GitHub templates
This template uses GitHub templates to provide checklists when making new pull requests. These templates are stored in the [.github/](./.github/) directory.

### VSCode
This template includes recommendations to VSCode users for extensions, particularly the `ruff` linter. These recommendations are stored in `.vscode/extensions.json`. When you open the repository in VSCode, you should see a prompt to install the recommended extensions.

### `.gitignore`
This template uses a `.gitignore` file to prevent certain files from being committed to the repository.

### `pyproject.toml`
`pyproject.toml` is a configuration file to specify your project's metadata and to set the behavior of other tools such as linters, type checkers etc. You can learn more [here](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/)

### Linting
This template automates linting and formatting using GitHub Actions and the `ruff` linter. When you push changes to your repository, GitHub will automatically run the linter and report any errors, blocking merges until they are resolved.
